{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "additional_stops = ['apple', 'm1', 'mac', 'new', 'rt', 'get', 'go', 'one', 'even', 'would',\n",
    "'macs', 'make', 'want', 'yes', 'really', 'could', 'say', 'lot', 'via', 'something', 'right',\n",
    "'since', 'give', 'hackintosh', 'ago', 'hi', 'ask', 'bo', 'probably', 'put', 'end', 'might', \n",
    "'around' 'us', 'happen', 'kill', 'use', 'mini', 'macbook']\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation + string.digits)\n",
    "stopwords_list += additional_stops\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "import GeneralFunctions\n",
    "\n",
    "\n",
    "def retrieve_batch(source_dir, ignore):\n",
    "    '''Retrieves a list of csv file names from source_dir(string) while ignoring\n",
    "    files named in ignore(list). The resulting list of files is read in and \n",
    "    concatenated into a larger data frame, or batch. Returns file_names(list), and\n",
    "    batch (pd.DataFrame).'''\n",
    "    # get all the files from the directory\n",
    "    file_names = [f for f in listdir(source_dir) if isfile(join(source_dir, f))]\n",
    "    file_names = [f for f in file_names if f not in ignore]\n",
    "    batch_name = ' '.join(str(datetime.now()).split(' '))[0:19].replace(':', '_').replace(' ','_')+'.csv'\n",
    "\n",
    "\n",
    "    # ignore secret files, establish a batch df, load each csv file, concatenate to batch, save batch\n",
    "    batch = pd.DataFrame()\n",
    "    for file_name in file_names:\n",
    "        data = pd.read_csv(source_dir + '/' + file_name)\n",
    "        print('[*] Adding ', file_name, 'to batch: ', batch_name)\n",
    "        batch = pd.concat([batch, data], axis=0)\n",
    "        \n",
    "    return batch\n",
    "\n",
    "\n",
    "def clean_text(text, tokenizer):\n",
    "    '''Accepts a lemmatizer (object), text (string), stopwords (list).\n",
    "    Tokenizes the text and removes usernames (^'@.*'), hashtags ('^#.*'),\n",
    "    and web addresses ('^http'). Then it lemmatizes the text and normalizes\n",
    "    to lowercase. Returns a list of modified text tokens.'''\n",
    "    \n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = [t for t in text if t.isalpha()]\n",
    "    text = [t.lower() for t in text]\n",
    "    remove = [t for t in text if t.startswith('@') or t.startswith('http') or t.startswith('www') or t.startswith('#')]\n",
    "    tokens = [t.replace('\\n', '') for t in text if t not in remove]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def process_text(text, stopwords_list, lemmatizer):\n",
    "    \n",
    "    text = text.split(' ')    \n",
    "    tokens = [t for t in text if t not in stopwords_list]\n",
    "    \n",
    "    return [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "\n",
    "\n",
    "def label_subjectivity(df):\n",
    "    \n",
    "    df['scores'] = df['cleaned_text'].apply(lambda x: TextBlob(x).sentiment)\n",
    "    df['subjectivity'] = df.scores.apply(lambda x: x[1])\n",
    "    df = df.drop('scores', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def label_polarity(df, analyzer):\n",
    "\n",
    "    df['scores'] = df.cleaned_text.apply(lambda x: 0 if type(x) == float else analyzer.polarity_scores(x))\n",
    "    df['pos'] = df.scores.apply(lambda x: x['pos'])\n",
    "    df['neg'] = df.scores.apply(lambda x: x['neg'])\n",
    "    df['neu'] = df.scores.apply(lambda x: x['neu'])\n",
    "    \n",
    "    df['com'] = df.scores.apply(lambda x: x['compound'])\n",
    "    \n",
    "    df['polarity'] = df.com.apply(lambda x: 1 if x > 0 else x)\n",
    "    df['polarity'] = df.polarity.apply(lambda x: -1 if x < 0 else x)\n",
    "    df = df.drop(['scores', 'pos', 'neg', 'neu'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def count_tags(text, tag):\n",
    "    return [t[1] for t in text if t[1] == tag]\n",
    "\n",
    "\n",
    "def pos_tag_data(df):\n",
    "    \n",
    "    df['pos_tags'] = pos_tag_sents(df['cleaned_text'].tolist())\n",
    "\n",
    "    all_tags = []\n",
    "    for tag in df.pos_tags:\n",
    "        all_tags += [t[1] for t in tag if t[1] not in all_tags]\n",
    "\n",
    "    for tag in all_tags:\n",
    "        df[tag] = df.pos_tags.apply(lambda x: count_tags(x, tag)).apply(lambda x: len(x))\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_data(platform, df, tokenizer, stopwords_list, lemmatizer, analyzer):\n",
    "    \n",
    "    print('\\n[*] Intiating text cleaning...')\n",
    "    print('-- Tokenizing...')\n",
    "    print('-- Removing non-alphabetic characters...')\n",
    "    print('-- Converting to lowercase...')\n",
    "    print('-- Removing hashtags, web addresses, and mentions...')\n",
    "    print('-- Removing any code tags...')\n",
    "    print('-- Joining tokens for further processing...')\n",
    "   \n",
    "    df = pd.DataFrame(df.text)\n",
    "\n",
    "    df['text'] = df.text.apply(lambda x: str(x))\n",
    "    df['cleaned_text'] = df['text'].apply(lambda x: clean_text(x, tokenizer))\n",
    "    \n",
    "    df['lens'] = df.cleaned_text.apply(lambda x: len(x))\n",
    "    df = df[df.lens > 0]\n",
    "    \n",
    "    print('\\n[*] Labeling subjectivity...')\n",
    "    print('-- Calculating subjectivity scores...')\n",
    "    df = label_subjectivity(df)\n",
    "    \n",
    "    print('\\n[*] Labeling polarity...')\n",
    "    print('-- Calculating polarity scores...')\n",
    "    print('-- Determining polarity label...')\n",
    "    df = label_polarity(df, analyzer)\n",
    "    \n",
    "    print('\\n[*] Processing text...')\n",
    "    print('-- Removing stopwords...')\n",
    "    print('-- Lemmatizing tokens...')\n",
    "    \n",
    "    df['cleaned_text'] = df.cleaned_text.apply(lambda x: process_text(x, stopwords_list, lemmatizer))\n",
    "    \n",
    "    print('\\n[*] Calculating text length...')\n",
    "    df['text_len'] = df.text.apply(lambda x: len(x.split(' ')))\n",
    "    \n",
    "    print('\\n[*] Applying POS tags...')\n",
    "    print('-- Obtaining POS tags...')\n",
    "    print('-- Creating POS tag list...')\n",
    "    print('-- Counting POS tags...')\n",
    "    df = pos_tag_data(df)\n",
    "    \n",
    "    print('\\n[*] Preprocessing Complete')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def batch_and_process_data(platform):\n",
    "    \n",
    "    GeneralFunctions.create_banner('Preprocess Data')\n",
    "    print('\\n[*] Obtaining data...\\n')\n",
    "    \n",
    "    ignore = ['.DS_Store']\n",
    "    twitter_source_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/tweet_data/raw_data/'\n",
    "    twitter_target_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/tweet_data/labeled_data/'\n",
    "    reddit_source_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/reddit_data/raw_data/session_data/'\n",
    "    reddit_target_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/reddit_data/labeled_data/'\n",
    "\n",
    "    if platform == 'Twitter':\n",
    "        print('\\n[*] Retrieving session data from source directory...\\n')\n",
    "        filenames, raw_data, batch_name = retrieve_batch(twitter_source_dir, ignore)\n",
    "        raw_data = raw_data.drop_duplicates(subset='user')\n",
    "        target_dir = twitter_target_dir\n",
    "\n",
    "    elif platform == 'Reddit':\n",
    "       \n",
    "        print('\\n[*] Retrieving session data from source directory...\\n')\n",
    "        filenames, raw_data, batch_name = retrieve_batch(reddit_source_dir, ignore)\n",
    "        raw_data = raw_data.drop_duplicates(subset='text')\n",
    "        target_dir = reddit_target_dir\n",
    "        \n",
    "    else:\n",
    "        print('\\n[*] Retrieving session data from source directory...\\n')\n",
    "        twitter_raw_data = retrieve_batch(twitter_source_dir, ignore)\n",
    "        twitter_raw_data.head()\n",
    "        twitter_raw_data = twitter_raw_data.drop_duplicates(subset='user')\n",
    "\n",
    "        reddit_raw_data = retrieve_batch(reddit_source_dir, ignore)\n",
    "        reddit_raw_data.head()\n",
    "        reddit_raw_data = reddit_raw_data.drop_duplicates(subset='text')\n",
    "        \n",
    "        raw_data = pd.concat([twitter_raw_data, reddit_raw_data], axis=0)\n",
    "        \n",
    "        batch_name = ' '.join(str(datetime.now()).split(' '))[0:19].replace(':', '_').replace(' ','_')+'.csv'  \n",
    "        target_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/combined_data/'\n",
    "        \n",
    "    print('\\n[*] Preprocessing batch:', batch_name)\n",
    "    data = process_data(platform, raw_data, tokenizer, stopwords_list, lemmatizer, analyzer)\n",
    "    \n",
    "    filename = ' '.join(str(datetime.now()).split(' '))[0:19].replace(':', '_').replace(' ','_')+'.csv'\n",
    "    csv_name = target_dir + filename\n",
    "    \n",
    "    print('\\n[*] Saving processed results to ', csv_name)\n",
    "    data.to_csv(csv_name, index=False)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Preprocess Data\n",
      "------------------------------\n",
      "\n",
      "[*] Obtaining data...\n",
      "\n",
      "\n",
      "[*] Retrieving session data from source directory...\n",
      "\n",
      "[*] Adding  2021-01-28_12_47_40.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-02-01_20_11_54.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_25.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_19.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_32_19.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_56_58.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_10_45.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_18.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_30_40.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-25_14_30_40.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_24.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_53_00.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_53_56.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_26.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_00_22.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_19_42.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_13_23.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-29_13_33_16.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_24_36.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_27.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_23.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_26_37.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_53_18.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_41_47.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_13_29_58.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_02_23.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_22.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_12_46.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_48_15.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-25_14_34_08.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_30_18.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_08.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_20.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_32_41.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_46_39.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_31_45.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_51_54.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_17_04_06.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_21.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_49_41.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_09.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_55_57.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_28_17.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_59_21.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_55_02.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_49_53.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_08_27.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-25_14_37_00.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_13_28_57.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_16_31.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_38_45.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_22_35.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_04_25.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_45_50.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_11_45.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_52_17.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_27_38.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_29_39.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_34_20.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_50_16.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_42_48.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_36_44.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_14_30.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-25_14_32_08.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_11_28.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_20_34.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_34_43.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_06_26.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_47_52.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_55_19.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_57_03.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_57_20.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_17_02_05.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_14_47.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_18_33.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_01_23.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_25_37.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_47_14.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_11_45.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-29_13_37_25.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_52_00.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-29_13_32_15.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_03_24.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_31_41.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_52_55.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_45_39.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_57_59.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-02-01_20_37_10.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_50_54.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_33_20.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_35_49.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_31_19.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_48_40.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-29_13_25_09.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_13_30.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-25_14_39_20.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-27_15_40_14.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_40_46.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_49_15.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_13_46.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_33_42.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_10.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_50_59.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_44_50.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-02-01_20_17_00.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-02-01_20_10_34.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_59_00.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_05_25.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_11.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_12_29.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_54_02.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_13.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_58_21.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_09_27.csv to batch:  2021-02-02_01_14_30.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Adding  2021-02-01_19_58_06.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_17_32.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_58_03.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_17_01_05.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_48_52.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-25_14_35_52.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_56_19.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_39_46.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_23_36.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_12.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_54_57.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_44_36.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_16.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_54_18.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_19_33.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_11_56_03.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_07_26.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_35_43.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-02-01_20_41_57.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_17.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_15.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_37_44.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_01_31.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_21_34.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_10_28.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_13_27_57.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_46_51.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_28_39.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_43_49.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_20_51_16.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_17_03_06.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_14.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-28_12_08_28.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-30_21_15_31.csv to batch:  2021-02-02_01_14_30.csv\n",
      "[*] Adding  2021-01-12_12_07_16.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-29_08_48_21.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-12_11_59_17.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-12_12_07_01.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-12_11_59_15.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-20_10_46_39.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-12_12_07_11.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-29_08_52_40.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-12_12_07_06.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-29_08_55_13.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-12_12_10_44.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-28_20_31_58.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-29_08_46_44.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-28_20_30_31.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-28_20_27_04.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-12_12_10_54.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-28_20_36_42.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-29_08_50_29.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-28_20_35_21.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-20_10_47_17.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-28_20_15_41.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-20_10_47_10.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-12_12_10_48.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-29_08_54_39.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-28_20_28_38.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-12_12_00_17.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-12_12_00_10.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-20_10_46_31.csv to batch:  2021-02-02_01_14_32.csv\n",
      "[*] Adding  2021-01-20_10_46_25.csv to batch:  2021-02-02_01_14_32.csv\n",
      "\n",
      "[*] Preprocessing batch: 2021-02-02_01_14_33.csv\n",
      "\n",
      "[*] Intiating text cleaning...\n",
      "-- Tokenizing...\n",
      "-- Removing non-alphabetic characters...\n",
      "-- Converting to lowercase...\n",
      "-- Removing hashtags, web addresses, and mentions...\n",
      "-- Removing any code tags...\n",
      "-- Joining tokens for further processing...\n",
      "\n",
      "[*] Labeling subjectivity...\n",
      "-- Calculating subjectivity scores...\n",
      "\n",
      "[*] Labeling polarity...\n",
      "-- Calculating polarity scores...\n",
      "-- Determining polarity label...\n",
      "\n",
      "[*] Processing text...\n",
      "-- Removing stopwords...\n",
      "-- Lemmatizing tokens...\n",
      "\n",
      "[*] Calculating text length...\n",
      "\n",
      "[*] Applying POS tags...\n",
      "-- Obtaining POS tags...\n",
      "-- Creating POS tag list...\n",
      "-- Counting POS tags...\n",
      "\n",
      "[*] Preprocessing Complete\n",
      "\n",
      "[*] Saving processed results to  /Users/christineegan/AppleM1SentimentAnalysis/data/combined_data/2021-02-02_01_14_46.csv\n"
     ]
    }
   ],
   "source": [
    "data = batch_and_process_data('both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/christineegan/AppleM1SentimentAnalysis/data/combined_data/2021-02-02_01_14_46.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lens</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>com</th>\n",
       "      <th>polarity</th>\n",
       "      <th>text_len</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>NN</th>\n",
       "      <th>JJ</th>\n",
       "      <th>...</th>\n",
       "      <th>RBS</th>\n",
       "      <th>NNP</th>\n",
       "      <th>RP</th>\n",
       "      <th>WP</th>\n",
       "      <th>PRP</th>\n",
       "      <th>WDT</th>\n",
       "      <th>WRB</th>\n",
       "      <th>WP$</th>\n",
       "      <th>TO</th>\n",
       "      <th>''</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>With the Apple M1 chip on MacBook Air and MacB...</td>\n",
       "      <td>['chip', 'air', 'pro', 'students', 'cpu', 'inc...</td>\n",
       "      <td>98</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "      <td>[('chip', 'NN'), ('air', 'NN'), ('pro', 'JJ'),...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @saraali16413422: #ParkJimin #KimNamjoon #S...</td>\n",
       "      <td>['best', 'laptop', 'chip', 'giant', 'leap', 'p...</td>\n",
       "      <td>53</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>[('best', 'JJS'), ('laptop', 'JJ'), ('chip', '...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @saraali16413422: #ParkJimin #KimNamjoon #S...</td>\n",
       "      <td>['best', 'laptop', 'chip', 'giant', 'leap', 'p...</td>\n",
       "      <td>53</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>[('best', 'JJS'), ('laptop', 'JJ'), ('chip', '...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @Bisma22148197: #ParkJimin #KimNamjoon #SUP...</td>\n",
       "      <td>['best', 'laptop', 'chip', 'giant', 'leap', 'p...</td>\n",
       "      <td>56</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20</td>\n",
       "      <td>[('best', 'JJS'), ('laptop', 'JJ'), ('chip', '...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hey! New #podcast Basics of #computer architec...</td>\n",
       "      <td>['hey', 'basics', 'architecture', 'feature', '...</td>\n",
       "      <td>76</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19</td>\n",
       "      <td>[('hey', 'NN'), ('basics', 'NNS'), ('architect...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2101</th>\n",
       "      <td>While I appreciate Apple’s fight for user priv...</td>\n",
       "      <td>['appreciate', 'fight', 'user', 'privacy', 'wi...</td>\n",
       "      <td>261</td>\n",
       "      <td>0.335658</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46</td>\n",
       "      <td>[('appreciate', 'NN'), ('fight', 'NN'), ('user...</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2102</th>\n",
       "      <td>Apple should make this statement at a serious ...</td>\n",
       "      <td>['statement', 'serious', 'security', 'conventi...</td>\n",
       "      <td>149</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.7184</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28</td>\n",
       "      <td>[('statement', 'NN'), ('serious', 'JJ'), ('sec...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2103</th>\n",
       "      <td>Committed to privacy but yet bans a free speec...</td>\n",
       "      <td>['commit', 'privacy', 'yet', 'ban', 'free', 's...</td>\n",
       "      <td>114</td>\n",
       "      <td>0.522222</td>\n",
       "      <td>0.8658</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22</td>\n",
       "      <td>[('commit', 'NN'), ('privacy', 'NN'), ('yet', ...</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2104</th>\n",
       "      <td>7zip file how do I open a 7 zip file without k...</td>\n",
       "      <td>['file', 'open', 'zip', 'file', 'without', 'kn...</td>\n",
       "      <td>125</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28</td>\n",
       "      <td>[('file', 'NN'), ('open', 'JJ'), ('zip', 'NN')...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2105</th>\n",
       "      <td>there is a password for a reasom</td>\n",
       "      <td>['password', 'reasom']</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[('password', 'NN'), ('reasom', 'NN')]</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2106 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     With the Apple M1 chip on MacBook Air and MacB...   \n",
       "1     RT @saraali16413422: #ParkJimin #KimNamjoon #S...   \n",
       "2     RT @saraali16413422: #ParkJimin #KimNamjoon #S...   \n",
       "3     RT @Bisma22148197: #ParkJimin #KimNamjoon #SUP...   \n",
       "4     Hey! New #podcast Basics of #computer architec...   \n",
       "...                                                 ...   \n",
       "2101  While I appreciate Apple’s fight for user priv...   \n",
       "2102  Apple should make this statement at a serious ...   \n",
       "2103  Committed to privacy but yet bans a free speec...   \n",
       "2104  7zip file how do I open a 7 zip file without k...   \n",
       "2105                   there is a password for a reasom   \n",
       "\n",
       "                                           cleaned_text  lens  subjectivity  \\\n",
       "0     ['chip', 'air', 'pro', 'students', 'cpu', 'inc...    98      0.900000   \n",
       "1     ['best', 'laptop', 'chip', 'giant', 'leap', 'p...    53      0.650000   \n",
       "2     ['best', 'laptop', 'chip', 'giant', 'leap', 'p...    53      0.650000   \n",
       "3     ['best', 'laptop', 'chip', 'giant', 'leap', 'p...    56      0.650000   \n",
       "4     ['hey', 'basics', 'architecture', 'feature', '...    76      0.527273   \n",
       "...                                                 ...   ...           ...   \n",
       "2101  ['appreciate', 'fight', 'user', 'privacy', 'wi...   261      0.335658   \n",
       "2102  ['statement', 'serious', 'security', 'conventi...   149      0.833333   \n",
       "2103  ['commit', 'privacy', 'yet', 'ban', 'free', 's...   114      0.522222   \n",
       "2104  ['file', 'open', 'zip', 'file', 'without', 'kn...   125      0.500000   \n",
       "2105                             ['password', 'reasom']    32      0.000000   \n",
       "\n",
       "         com  polarity  text_len  \\\n",
       "0     0.0000       0.0        21   \n",
       "1     0.6369       1.0        20   \n",
       "2     0.6369       1.0        20   \n",
       "3     0.6369       1.0        20   \n",
       "4     0.4404       1.0        19   \n",
       "...      ...       ...       ...   \n",
       "2101  0.5719       1.0        46   \n",
       "2102  0.7184       1.0        28   \n",
       "2103  0.8658       1.0        22   \n",
       "2104  0.0000       0.0        28   \n",
       "2105  0.0000       0.0         7   \n",
       "\n",
       "                                               pos_tags  NN  JJ  ...  RBS  \\\n",
       "0     [('chip', 'NN'), ('air', 'NN'), ('pro', 'JJ'),...   4   2  ...    0   \n",
       "1     [('best', 'JJS'), ('laptop', 'JJ'), ('chip', '...   3   1  ...    0   \n",
       "2     [('best', 'JJS'), ('laptop', 'JJ'), ('chip', '...   3   1  ...    0   \n",
       "3     [('best', 'JJS'), ('laptop', 'JJ'), ('chip', '...   2   3  ...    0   \n",
       "4     [('hey', 'NN'), ('basics', 'NNS'), ('architect...   2   2  ...    0   \n",
       "...                                                 ...  ..  ..  ...  ...   \n",
       "2101  [('appreciate', 'NN'), ('fight', 'NN'), ('user...  12   6  ...    0   \n",
       "2102  [('statement', 'NN'), ('serious', 'JJ'), ('sec...   8   1  ...    0   \n",
       "2103  [('commit', 'NN'), ('privacy', 'NN'), ('yet', ...   7   3  ...    0   \n",
       "2104  [('file', 'NN'), ('open', 'JJ'), ('zip', 'NN')...   9   1  ...    0   \n",
       "2105             [('password', 'NN'), ('reasom', 'NN')]   2   0  ...    0   \n",
       "\n",
       "      NNP  RP  WP  PRP  WDT  WRB  WP$  TO  ''  \n",
       "0       0   0   0    0    0    0    0   0   0  \n",
       "1       0   0   0    0    0    0    0   0   0  \n",
       "2       0   0   0    0    0    0    0   0   0  \n",
       "3       0   0   0    0    0    0    0   0   0  \n",
       "4       0   0   0    0    0    0    0   0   0  \n",
       "...   ...  ..  ..  ...  ...  ...  ...  ..  ..  \n",
       "2101    0   0   0    0    0    0    0   0   0  \n",
       "2102    0   0   0    0    0    0    0   0   0  \n",
       "2103    0   0   0    0    0    0    0   0   0  \n",
       "2104    0   0   0    0    0    0    0   0   0  \n",
       "2105    0   0   0    0    0    0    0   0   0  \n",
       "\n",
       "[2106 rows x 39 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apple_m1_env",
   "language": "python",
   "name": "apple_m1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

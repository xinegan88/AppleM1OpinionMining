{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def batch_and_process_data(platform: str) -> (pd.DataFrame):\n",
    "    '''Data for the provided platform is retrieved and batched. The batch \n",
    "    is saved in its raw form, and then processed and returned.\n",
    "    '''\n",
    "    general_functions.create_banner('Preprocess Data')\n",
    "    \n",
    "    ignore = ['.DS_Store']\n",
    "    t_source_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/tweet_data/raw_data/'\n",
    "    t_target_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/tweet_data/labeled_data/'\n",
    "    r_source_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/reddit_data/raw_data/session_data/'\n",
    "    r_target_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/reddit_data/labeled_data/'\n",
    "    \n",
    "    return t_source_dir, t_target_dir, r_source_dir, r_target_dir\n",
    "\n",
    "\n",
    "def date_directory(target_dir: str) -> (str):\n",
    "    '''Checks the target directory to see if the date stamped directory exists\n",
    "    for the current date. If no directory exists, it is created and the name of\n",
    "    the new directory is returned.\n",
    "    '''\n",
    "    dir = os.path.join(target_dir+str(datetime.date(datetime.now())))\n",
    "    if not os.path.exists(dir):\n",
    "        os.mkdir(dir)\n",
    "    return str(dir+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/christineegan/AppleM1SentimentAnalysis/data/tweet_data/labeled_data/2021-02-07/\n"
     ]
    }
   ],
   "source": [
    "target_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/tweet_data/labeled_data/'\n",
    "date_dir = date_directory(target_dir)\n",
    "print(date_dir+'/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "from nltk import pos_tag, pos_tag_sents\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "additional_stops = ['apple', 'm1', 'mac', 'new', 'rt', 'get', 'go', 'one', 'even', 'would',\n",
    "'macs', 'make', 'want', 'yes', 'really', 'could', 'say', 'lot', 'via', 'something', 'right',\n",
    "'since', 'give', 'hackintosh', 'ago', 'hi', 'ask', 'bo', 'probably', 'put', 'end', 'might', \n",
    "'around' 'us', 'happen', 'kill', 'use', 'mini', 'macbook']\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list += list(string.punctuation + string.digits)\n",
    "stopwords_list += additional_stops\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "import general_functions\n",
    "\n",
    "\n",
    "def clean_text(text: str, tokenizer: str) -> (str):\n",
    "    '''Accepts a lemmatizer, text, stopwords. Tokenizes the text and \n",
    "    removes usernames, hashtags, and web addresses. Then it lemmatizes \n",
    "    the text and normalizes to lowercase. Returns a list of modified text\n",
    "    tokens.\n",
    "    '''\n",
    "    text = tokenizer.tokenize(text)\n",
    "    text = [t for t in text if t.isalpha()]\n",
    "    text = [t.lower() for t in text]\n",
    "    remove = [t for t in text if t.startswith('@') or t.startswith('#')]\n",
    "    remove += [t for t in text if t.startswith('http') or t.startswith('www')]\n",
    "    tokens = [t.replace('\\n', '') for t in text if t not in remove]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def process_text(text: str, stopwords_list: list, \n",
    "                 lemmatizer: str) -> (list):\n",
    "    '''Accepts text, stopwords, and a lemmatizer, removes stopwords and\n",
    "    lemmatizes text.\n",
    "    '''\n",
    "    text = text.split(' ')    \n",
    "    tokens = [t for t in text if t not in stopwords_list]\n",
    "    \n",
    "    return [lemmatizer.lemmatize(token, pos='v') for token in tokens]\n",
    "\n",
    "\n",
    "def label_subjectivity(df: pd.DataFrame) -> (pd.DataFrame):\n",
    "    '''Accepts a dataf frame witha  cleaned text features, and applies a\n",
    "    TextBlob subjectivity score.\n",
    "    '''\n",
    "    df['scores'] = df['cleaned_text'].apply(lambda x: TextBlob(x).sentiment)\n",
    "    df['subjectivity'] = df.scores.apply(lambda x: x[1])\n",
    "    df = df.drop('scores', axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def label_polarity(df: pd.DataFrame, analyzer: str) -> (pd.DataFrame):\n",
    "    '''Accepts a dataf frame witha  cleaned text features, and calculates\n",
    "    a set of polarity scores. The polarity scores is divided, and all \n",
    "    scores are dropped except for the compound score. The compound score is\n",
    "    classified as -1, 0, or 1.\n",
    "    '''\n",
    "    df['scores'] = df.cleaned_text.apply(lambda x: 0 if type(x) == float else analyzer.polarity_scores(x))\n",
    "    df['pos'] = df.scores.apply(lambda x: x['pos'])\n",
    "    df['neg'] = df.scores.apply(lambda x: x['neg'])\n",
    "    df['neu'] = df.scores.apply(lambda x: x['neu'])\n",
    "    df['com'] = df.scores.apply(lambda x: x['compound'])\n",
    "    df['polarity'] = df.com.apply(lambda x: 1 if x > 0 else x)\n",
    "    df['polarity'] = df.polarity.apply(lambda x: -1 if x < 0 else x)\n",
    "    df = df.drop(['scores', 'pos', 'neg', 'neu'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def count_tags(text: list, tag: str) -> (list):\n",
    "    '''Accepts a list of a tuples that contain a word and its POS tag.\n",
    "    Returns a list of only the tags.\n",
    "    '''\n",
    "    return [t[1] for t in text if t[1] == tag]\n",
    "\n",
    "\n",
    "def pos_tag_data(df: pd.DataFrame) -> (pd.DataFrame):\n",
    "    '''Accepts a pd.DataFrame with a POS tag feature and applies\n",
    "    NLTK pos_tag_sents to the values. Compiles a list of tags used\n",
    "    to create a unique column for each POS tag for dummy encoding.\n",
    "    Returns the transformed data frame.\n",
    "    '''    \n",
    "    df['pos_tags'] = pos_tag_sents(df['cleaned_text'].tolist())\n",
    "\n",
    "    all_tags = []\n",
    "    for tag in df.pos_tags:\n",
    "        all_tags += [t[1] for t in tag if t[1] not in all_tags]\n",
    "\n",
    "    for tag in all_tags:\n",
    "        df[tag] = df.pos_tags.apply(lambda x: count_tags(x, \n",
    "                              tag)).apply(lambda x: len(x))\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_data(df: pd.DataFrame, tokenizer: str, stopwords_list: list,\n",
    "                 lemmatizer: str, analyzer: str) -> (pd.DataFrame):\n",
    "    '''\n",
    "    Accepts a pd.DataFrame and parameters. Applies, tokenization to\n",
    "    the text data. Removes stopwords. Then analyzes the text to apply\n",
    "    sentiment labels (subjectivity and polarity). Next, text is lemmatized.\n",
    "    The transformed data frame is returned.\n",
    "    ''' \n",
    "    print('\\n[*] Intiating text cleaning...')\n",
    "    print('-- Tokenizing...')\n",
    "    print('-- Removing non-alphabetic characters...')\n",
    "    print('-- Converting to lowercase...')\n",
    "    print('-- Removing hashtags, web addresses, and mentions...')\n",
    "    print('-- Removing any code tags...')\n",
    "    print('-- Joining tokens for further processing...')\n",
    "    df = pd.DataFrame(df.text)\n",
    "    df['text'] = df.text.apply(lambda x: str(x))\n",
    "    df['cleaned_text'] = df['text'].apply(lambda x: clean_text(x, tokenizer))\n",
    "    df['lens'] = df.cleaned_text.apply(lambda x: len(x))\n",
    "    df = df[df.lens > 0]\n",
    "    \n",
    "    print('\\n[*] Labeling subjectivity...')\n",
    "    print('-- Calculating subjectivity scores...')\n",
    "    df = label_subjectivity(df)\n",
    "    \n",
    "    print('\\n[*] Labeling polarity...')\n",
    "    print('-- Calculating polarity scores...')\n",
    "    print('-- Determining polarity label...')\n",
    "    df = label_polarity(df, analyzer)\n",
    "    \n",
    "    answers = ['Y', 'N']\n",
    "    print('Would you like to eliminate the neutral class?', answers)\n",
    "    answer = input()\n",
    "    \n",
    "    while answer.upper() not in answers:\n",
    "        print('Input not recognized. Please try again.')\n",
    "    if answer.upper() == answers[0]:\n",
    "        df = df[df.polarity != 0]\n",
    "\n",
    "    print('\\n[*] Processing text...')\n",
    "    print('-- Removing stopwords...')\n",
    "    print('-- Lemmatizing tokens...')\n",
    "    df['cleaned_text'] = df.cleaned_text.apply(lambda x: process_text(x, stopwords_list, lemmatizer))\n",
    "    \n",
    "    print('\\n[*] Calculating text length...')\n",
    "    df['text_len'] = df.text.apply(lambda x: len(x.split(' ')))\n",
    "    \n",
    "    print('\\n[*] Applying POS tags...')\n",
    "    print('-- Obtaining POS tags...')\n",
    "    print('-- Creating POS tag list...')\n",
    "    print('-- Counting POS tags...')\n",
    "    df = pos_tag_data(df)\n",
    "    \n",
    "    print('\\n[*] Preprocessing Complete')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def batch_and_process_data(platform: str) -> (pd.DataFrame):\n",
    "    '''Data for the provided platform is retrieved and batched. The batch \n",
    "    is saved in its raw form, and then processed and returned.\n",
    "    '''\n",
    "    general_functions.create_banner('Preprocess Data')\n",
    "    \n",
    "    ignore = ['.DS_Store']\n",
    "    t_source_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/tweet_data/raw_data/'\n",
    "    t_target_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/tweet_data/labeled_data/'\n",
    "    r_source_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/reddit_data/raw_data/session_data/'\n",
    "    r_target_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/reddit_data/labeled_data/'\n",
    "\n",
    "    if platform == 'Twitter':\n",
    "        print('[*] Retrieving session data from source directory...\\n')\n",
    "        file_names, raw_data, batch_name = general_functions.retrieve_batch(t_source_dir, ignore)\n",
    "        target_dir = t_target_dir\n",
    "\n",
    "    elif platform == 'Reddit':\n",
    "        print('[*] Retrieving session data from source directory...\\n')\n",
    "        file_names, raw_data, batch_name = general_functions.retrieve_batch(r_source_dir, ignore)\n",
    "        target_dir = r_target_dir\n",
    "        \n",
    "    else:\n",
    "        print('[*] Retrieving session data from source directory...\\n')\n",
    "        t_file_names, t_raw_data, t_batch_name = general_functions.retrieve_batch(t_source_dir, ignore)\n",
    "        r_filenames, r_raw_data, r_batch_name = retrieve_batch(t_source_dir, ignore)\n",
    "        raw_data = pd.concat([twitter_raw_data, reddit_raw_data], axis=0)\n",
    "        batch_name = ' '.join(str(datetime.now()).split(' '))[0:19]\n",
    "        batch_name = batch_name.replace(':', '_').replace(' ','_')+'.csv'  \n",
    "        target_dir = '/Users/christineegan/AppleM1SentimentAnalysis/data/combined_data/'\n",
    "        \n",
    "    print('\\n[*] Preprocessing batch:', batch_name)\n",
    "    data = process_data(raw_data, tokenizer, stopwords_list, lemmatizer, analyzer)\n",
    "    \n",
    "    date_dir = date_directory(target_dir)\n",
    "    print(date_dir+'/')\n",
    "\n",
    "    file_name = ' '.join(str(datetime.now()).split(' '))[0:19]\n",
    "    file_name = file_name.replace(':', '_').replace(' ','_')+'.csv'\n",
    "    csv_name = date_target_dir + file_name\n",
    "    \n",
    "    print('\\n[*] Saving processed results to ', csv_name)\n",
    "    data.to_csv(csv_name, index=False)\n",
    "        \n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apple_m1_env",
   "language": "python",
   "name": "apple_m1_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
